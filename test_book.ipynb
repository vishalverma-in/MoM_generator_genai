{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "from llm import llm_gemini"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "llm_gemini = llm_gemini()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "J'adore la programmation.\n"
     ]
    }
   ],
   "source": [
    "messages = [\n",
    "    (\n",
    "        \"system\",\n",
    "        \"You are a helpful assistant that translates English to French. Translate the user sentence.\",\n",
    "    ),\n",
    "    (\"human\", \"I love programming.\"),\n",
    "]\n",
    "ai_msg = llm_gemini.invoke(messages)\n",
    "print(ai_msg.content)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "<bound method BaseMessage.text of AIMessage(content=\"J'adore la programmation.\", additional_kwargs={}, response_metadata={'prompt_feedback': {'block_reason': 0, 'safety_ratings': []}, 'finish_reason': 'STOP', 'safety_ratings': []}, id='run-8acc3d4b-327e-483b-8758-e93b19e8c7b9-0', usage_metadata={'input_tokens': 20, 'output_tokens': 7, 'total_tokens': 27, 'input_token_details': {'cache_read': 0}})>\n"
     ]
    }
   ],
   "source": [
    "print(ai_msg.text)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'input_tokens': 20,\n",
       " 'output_tokens': 7,\n",
       " 'total_tokens': 27,\n",
       " 'input_token_details': {'cache_read': 0}}"
      ]
     },
     "execution_count": 20,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "ai_msg.usage_metadata"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "import google.generativeai as genai\n",
    "import os"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "GEMINI_API_KEY = os.getenv(\"GEMINI_API_KEY\")\n",
    "genai.configure(api_key=GEMINI_API_KEY)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [],
   "source": [
    "# load trasncript summarizer\n",
    "llm_model = genai.GenerativeModel('gemini-2.0-flash')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "J'adore la programmation.\n",
      "\n"
     ]
    }
   ],
   "source": [
    "prompt = f\"\"\"\n",
    "You are a helpful assistant that translates English to French. Translate the user sentence.\n",
    "User: \"I love programming.\"\n",
    "\"\"\"\n",
    "response = llm_model.generate_content(prompt)\n",
    "print(response.text)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "prompt_token_count: 26\n",
       "candidates_token_count: 7\n",
       "total_token_count: 33"
      ]
     },
     "execution_count": 19,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "response.usage_metadata"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from langchain_core.prompts import ChatPromptTemplate\n",
    "\n",
    "prompt = ChatPromptTemplate.from_messages(\n",
    "    [\n",
    "        (\n",
    "            \"system\",\n",
    "            \"You are a helpful assistant that translates {input_language} to {output_language}.\",\n",
    "        ),\n",
    "        (\"human\", \"{input}\"),\n",
    "    ]\n",
    ")\n",
    "\n",
    "chain = prompt | llm\n",
    "chain.invoke(\n",
    "    {\n",
    "        \"input_language\": \"English\",\n",
    "        \"output_language\": \"German\",\n",
    "        \"input\": \"I love programming.\",\n",
    "    }\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [],
   "source": [
    "transcript = \"\"\"\n",
    "Speaker 1: Let's start with the project timeline. We need to finalize the deadline.\n",
    "Speaker 2: Agreed. I suggest we set it for the end of next month.\n",
    "Speaker 1: That works. John, can you handle the design phase?\n",
    "Speaker 3: Yes, I'll take care of it.\n",
    "Speaker 2: Great. Let's meet again next week to review progress.\n",
    "\"\"\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [],
   "source": [
    "from langchain_core.prompts import ChatPromptTemplate"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [],
   "source": [
    "prompt = ChatPromptTemplate.from_messages(\n",
    "[\n",
    "    (\n",
    "        \"system\",\n",
    "        \"\"\"\n",
    "        You are an expert in summarizing meeting transcripts. Analyze the following transcript and generate Minutes of the Meeting (MoM). Include:\n",
    "        1. Key discussion points.\n",
    "        2. Decisions made.\n",
    "        3. Action items with assigned owners.\n",
    "        Format the output clearly with headings and bullet points. Include references to the speakers where relevant.\n",
    "        \"\"\",\n",
    "    ),\n",
    "    (\"human\", \"{transcript}\"),\n",
    "]\n",
    ")\n",
    "\n",
    "chain = prompt | llm_gemini\n",
    "\n",
    "response = chain.invoke(\n",
    "    {\n",
    "        \"transcript\": transcript,\n",
    "    }\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "## Minutes of Meeting\n",
      "\n",
      "**Date:** [Insert Date]\n",
      "**Attendees:** Speaker 1, Speaker 2, John (Speaker 3)\n",
      "\n",
      "**1. Key Discussion Points:**\n",
      "\n",
      "*   **Project Timeline:** Discussion focused on finalizing the project deadline.\n",
      "*   **Design Phase:** Assignment of responsibility for the design phase.\n",
      "*   **Follow-up Meeting:** Agreement to hold a follow-up meeting to review progress.\n",
      "\n",
      "**2. Decisions Made:**\n",
      "\n",
      "*   **Project Deadline:** The project deadline was set for the end of next month (Speaker 2).\n",
      "*   **Design Phase Owner:** John (Speaker 3) was assigned ownership of the design phase.\n",
      "\n",
      "**3. Action Items:**\n",
      "\n",
      "*   **Design Phase Completion:** John (Speaker 3) to complete the design phase.\n",
      "*   **Schedule Follow-up Meeting:** Speaker 2 to schedule a follow-up meeting for next week to review progress.\n"
     ]
    }
   ],
   "source": [
    "print(response.content)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "myenv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.1"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
